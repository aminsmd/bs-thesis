{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import keras \n",
    "from keras.layers import Dense, Embedding, LSTM, Input\n",
    "import matplotlib.pyplot as plt \n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import gensim\n",
    "import pickle as pk\n",
    "from io import StringIO\n",
    "from scipy import spatial\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import LSTM,Dense,Input,Bidirectional, Embedding, Dropout\n",
    "from keras import regularizers\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "from scipy import spatial\n",
    "from random import shuffle\n",
    "import pickle as pk\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.text import one_hot\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([pd.read_csv('fp.csv'), pd.read_csv('bc.csv'), pd.read_csv('chr.csv')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3635"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['Unnamed: 0','polarity'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>factuality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I have been a coffee drinker on and off since ...</td>\n",
       "      <td>EXPERIENCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I notice now that I am older, and perhaps I ha...</td>\n",
       "      <td>OPINION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>When I drink it, if I drink about 2 cups a day...</td>\n",
       "      <td>EXPERIENCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I always kind of said oh it's just allergies f...</td>\n",
       "      <td>OPINION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>but in fact when I do not consume coffee, I am...</td>\n",
       "      <td>EXPERIENCE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  factuality\n",
       "0  I have been a coffee drinker on and off since ...  EXPERIENCE\n",
       "1  I notice now that I am older, and perhaps I ha...     OPINION\n",
       "2  When I drink it, if I drink about 2 cups a day...  EXPERIENCE\n",
       "3  I always kind of said oh it's just allergies f...     OPINION\n",
       "4  but in fact when I do not consume coffee, I am...  EXPERIENCE"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df['factuality'] != 'NOT_LABELED']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3292"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(df.text, df.factuality, test_size = 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       ...,\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn = LabelBinarizer()\n",
    "bn.fit(Y_train)\n",
    "Y_train_bin = bn.transform(Y_train)\n",
    "Y_test_bin = bn.transform(Y_test)\n",
    "Y_train_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = 0\n",
    "for i in df.text.str.split():\n",
    "    m = max(m, len(i))\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_data = tokenizer.texts_to_sequences(X_train)\n",
    "x_train = pad_sequences(sequences_data, maxlen= 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_data_test = tokenizer.texts_to_sequences(X_test)\n",
    "x_test = pad_sequences(sequences_data_test, maxlen= 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format('/Users/amin/Downloads/w2v_phrase3_npmi.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gorgeous', 0.5672570466995239),\n",
       " ('cute', 0.5565539598464966),\n",
       " ('romantic', 0.53650963306427),\n",
       " ('attractive', 0.5300488471984863),\n",
       " ('jealous', 0.5028330683708191),\n",
       " ('intelligent', 0.49756476283073425),\n",
       " ('naked', 0.49107375741004944),\n",
       " ('shy', 0.4861142635345459),\n",
       " ('charming', 0.48583555221557617),\n",
       " ('aroused', 0.48257148265838623)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.most_similar('sexy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "N_CLASSES = 3\n",
    "embedding_dim = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null word embeddings: 0\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "nb_words = len(word_index)+1\n",
    "\n",
    "embedding_matrix = np.random.rand(nb_words, EMBEDDING_DIM)\n",
    "for word, i in word_index.items():\n",
    "    if word in word2vec.vocab:\n",
    "        embedding_matrix[i] = word2vec.word_vec(word)\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "\n",
    "embedding_layer = Embedding(embedding_matrix.shape[0], # or len(word_index) + 1\n",
    "                            embedding_matrix.shape[1], # or EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5303, 300)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 100, 300)          1590900   \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 100, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 100, 80)           109120    \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 100, 80)           38720     \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 80)                38720     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                810       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 33        \n",
      "=================================================================\n",
      "Total params: 1,778,303\n",
      "Trainable params: 187,403\n",
      "Non-trainable params: 1,590,900\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Input, Flatten, LSTM, Dropout, Bidirectional\n",
    "from keras.layers import GlobalAveragePooling1D, Embedding , Reshape,Concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.convolutional import Conv2D,MaxPooling2D\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "#filter_sizes = [2,3,4,5,6]\n",
    "drop = 0.2\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "reshape = Reshape((MAX_SEQUENCE_LENGTH,EMBEDDING_DIM))(embedded_sequences)\n",
    "lstm_0 = Bidirectional(LSTM(activation='tanh',units=40, return_sequences=True))(reshape)\n",
    "lstm_1 = Bidirectional(LSTM(activation='tanh',units=40, return_sequences=True))(lstm_0)\n",
    "lstm_2 = Bidirectional(LSTM(activation='tanh',units=40, dropout=drop))(lstm_1)\n",
    "\n",
    "dense_0 = Dense(10,activation='tanh')(lstm_2)\n",
    "out = Dense(N_CLASSES,activation='softmax')(dense_0)\n",
    "\n",
    "model = Model(inputs=sequence_input, outputs=out)\n",
    "#adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = './weights/factuality/lstm_bidir/weights_lstm_bidir-{epoch:02d}-{val_acc:.4f}.hdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(fp, monitor='val_acc', verbose=1, save_best_only=True, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 2798 samples, validate on 494 samples\n",
      "Epoch 1/10\n",
      "2798/2798 [==============================] - 87s 31ms/step - loss: 0.7959 - acc: 0.6494 - val_loss: 0.6741 - val_acc: 0.7105\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.71053, saving model to ./weights/factuality/lstm_bidir/weights_lstm_bidir-01-0.7105.hdf\n",
      "Epoch 2/10\n",
      "2798/2798 [==============================] - 78s 28ms/step - loss: 0.5767 - acc: 0.7734 - val_loss: 0.6564 - val_acc: 0.7389\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.71053 to 0.73887, saving model to ./weights/factuality/lstm_bidir/weights_lstm_bidir-02-0.7389.hdf\n",
      "Epoch 3/10\n",
      "2798/2798 [==============================] - 55s 20ms/step - loss: 0.4336 - acc: 0.8427 - val_loss: 0.7439 - val_acc: 0.7206\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.73887\n",
      "Epoch 4/10\n",
      "2798/2798 [==============================] - 64s 23ms/step - loss: 0.3194 - acc: 0.8921 - val_loss: 0.7220 - val_acc: 0.7591\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.73887 to 0.75911, saving model to ./weights/factuality/lstm_bidir/weights_lstm_bidir-04-0.7591.hdf\n",
      "Epoch 5/10\n",
      "2798/2798 [==============================] - 64s 23ms/step - loss: 0.2376 - acc: 0.9242 - val_loss: 0.7805 - val_acc: 0.7470\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.75911\n",
      "Epoch 6/10\n",
      "2798/2798 [==============================] - 53s 19ms/step - loss: 0.1535 - acc: 0.9521 - val_loss: 0.8660 - val_acc: 0.7470\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.75911\n",
      "Epoch 7/10\n",
      "2798/2798 [==============================] - 51s 18ms/step - loss: 0.1302 - acc: 0.9610 - val_loss: 1.0086 - val_acc: 0.7024\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.75911\n",
      "Epoch 8/10\n",
      "2798/2798 [==============================] - 51s 18ms/step - loss: 0.1233 - acc: 0.9607 - val_loss: 0.9487 - val_acc: 0.7227\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.75911\n",
      "Epoch 9/10\n",
      "2798/2798 [==============================] - 63s 23ms/step - loss: 0.0643 - acc: 0.9818 - val_loss: 1.0902 - val_acc: 0.7348\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.75911\n",
      "Epoch 10/10\n",
      "2798/2798 [==============================] - 81s 29ms/step - loss: 0.0707 - acc: 0.9800 - val_loss: 1.1540 - val_acc: 0.7409\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.75911\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1399fde10>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, Y_train_bin, validation_data=[x_test,Y_test_bin], nb_epoch=10, batch_size=16, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      0.87      0.85       237\n",
      "          1       0.62      0.63      0.62       109\n",
      "          2       0.76      0.68      0.71       148\n",
      "\n",
      "avg / total       0.76      0.76      0.76       494\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model.load_weights('./weights/factuality/lstm_bidir/weights_lstm_bidir-04-0.7591.hdf')\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "output_test = np.argmax(y_pred, axis=1)\n",
    "print(classification_report(np.argmax(Y_test_bin, axis=1), output_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 100, 300)          1590900   \n",
      "_________________________________________________________________\n",
      "reshape_3 (Reshape)          (None, 100, 300)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 98, 32)            28832     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 49, 32)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_7 (Bidirection (None, 49, 80)            23360     \n",
      "_________________________________________________________________\n",
      "bidirectional_8 (Bidirection (None, 49, 80)            38720     \n",
      "_________________________________________________________________\n",
      "bidirectional_9 (Bidirection (None, 80)                38720     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 80)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 20)                1620      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 3)                 63        \n",
      "=================================================================\n",
      "Total params: 1,722,215\n",
      "Trainable params: 131,315\n",
      "Non-trainable params: 1,590,900\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Input, Flatten, LSTM, Dropout, Bidirectional\n",
    "from keras.layers import GlobalAveragePooling1D, Embedding , Reshape,Concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.convolutional import Conv2D,MaxPooling2D,Conv1D,MaxPooling1D\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "#filter_sizes = [2,3,4,5,6]\n",
    "drop = 0.5\n",
    "num_filters = 32\n",
    "\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "reshape = Reshape((MAX_SEQUENCE_LENGTH,EMBEDDING_DIM))(embedded_sequences)\n",
    "\n",
    "conv_0 = Conv1D(num_filters, kernel_size=3, padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "maxpool_0 = MaxPooling1D(pool_size=2)(conv_0)\n",
    "\n",
    "#print(maxpool_0.shape)\n",
    "\n",
    "#reshape = Reshape((MAX_SEQUENCE_LENGTH,EMBEDDING_DIM))(embedded_sequences)\n",
    "\n",
    "lstm_0 = Bidirectional(LSTM(activation='tanh',units=40, return_sequences=True))(maxpool_0)\n",
    "lstm_1 = Bidirectional(LSTM(activation='tanh',units=40, return_sequences=True))(lstm_0)\n",
    "lstm_2 = Bidirectional(LSTM(activation='tanh',units=40))(lstm_1)\n",
    "\n",
    "dropout = Dropout(drop)(lstm_2)\n",
    "\n",
    "dense_0 = Dense(20,activation='tanh')(dropout)\n",
    "dense_1 = Dense(10,activation='tanh')(dense_0)\n",
    "out = Dense(N_CLASSES,activation='softmax')(dense_0)\n",
    "\n",
    "model_cnn_lstm = Model(inputs=sequence_input, outputs=out)\n",
    "#adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model_cnn_lstm.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model_cnn_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = './weights/factuality/cnn_lstm/weights_cnn_lstm-{epoch:02d}-{val_acc:.4f}.hdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(fp, monitor='val_acc', verbose=1, save_best_only=True, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn_lstm.load_weights('./weights/factuality/cnn_lstm/weights_cnn_lstm-01-0.7510.hdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2798 samples, validate on 494 samples\n",
      "Epoch 1/10\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.5121 - acc: 0.8081 - val_loss: 0.6607 - val_acc: 0.7449\n",
      "\n",
      "Epoch 00001: val_acc did not improve from 0.75101\n",
      "Epoch 2/10\n",
      "2798/2798 [==============================] - 25s 9ms/step - loss: 0.4073 - acc: 0.8528 - val_loss: 0.6653 - val_acc: 0.7490\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.75101\n",
      "Epoch 3/10\n",
      "2798/2798 [==============================] - 26s 9ms/step - loss: 0.3169 - acc: 0.8999 - val_loss: 0.7862 - val_acc: 0.7328\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.75101\n",
      "Epoch 4/10\n",
      " 944/2798 [=========>....................] - ETA: 17s - loss: 0.2169 - acc: 0.9322"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-3cac5251271c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_cnn_lstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train_bin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_test_bin\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_cnn_lstm.fit(x_train, Y_train_bin, validation_data=[x_test,Y_test_bin], nb_epoch=10, batch_size=16, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.90      0.84       237\n",
      "          1       0.66      0.55      0.60       109\n",
      "          2       0.73      0.66      0.69       148\n",
      "\n",
      "avg / total       0.74      0.75      0.74       494\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_cnn_lstm.load_weights('./weights/factuality/cnn_lstm/weights_cnn_lstm-01-0.7510.hdf')\n",
    "\n",
    "y_pred = model_cnn_lstm.predict(x_test)\n",
    "\n",
    "output_test = np.argmax(y_pred, axis=1)\n",
    "print(classification_report(np.argmax(Y_test_bin, axis=1), output_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 100, 300)     1590900     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 100, 300, 1)  0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 99, 1, 32)    19232       reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 98, 1, 32)    28832       reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 97, 1, 32)    38432       reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 96, 1, 32)    48032       reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 1, 1, 32)     0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 1, 1, 32)     0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 1, 1, 32)     0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 1, 1, 32)     0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 4, 1, 32)     0           max_pooling2d_1[0][0]            \n",
      "                                                                 max_pooling2d_2[0][0]            \n",
      "                                                                 max_pooling2d_3[0][0]            \n",
      "                                                                 max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 128)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 128)          0           flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 3)            387         dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,725,815\n",
      "Trainable params: 134,915\n",
      "Non-trainable params: 1,590,900\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import GlobalAveragePooling1D, Embedding , Reshape,Concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.convolutional import Conv2D,MaxPooling2D\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "num_filters = 32\n",
    "filter_sizes = [2,3,4,5]\n",
    "drop = 0.4\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "# print(embedded_sequences.shape)\n",
    "reshape = Reshape((MAX_SEQUENCE_LENGTH,EMBEDDING_DIM,1))(embedded_sequences)\n",
    "# print(reshape.shape)\n",
    "\n",
    "conv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "conv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[1], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "conv_2 = Conv2D(num_filters, kernel_size=(filter_sizes[2], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "conv_3 = Conv2D(num_filters, kernel_size=(filter_sizes[3], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "\n",
    "maxpool_0 = MaxPooling2D(pool_size=(MAX_SEQUENCE_LENGTH - filter_sizes[0] + 1, 1), strides=(1,1), padding='valid')(conv_0)\n",
    "maxpool_1 = MaxPooling2D(pool_size=(MAX_SEQUENCE_LENGTH - filter_sizes[1] + 1, 1), strides=(1,1), padding='valid')(conv_1)\n",
    "maxpool_2 = MaxPooling2D(pool_size=(MAX_SEQUENCE_LENGTH - filter_sizes[2] + 1, 1), strides=(1,1), padding='valid')(conv_2)\n",
    "maxpool_3 = MaxPooling2D(pool_size=(MAX_SEQUENCE_LENGTH - filter_sizes[3] + 1, 1), strides=(1,1), padding='valid')(conv_3)\n",
    "\n",
    "\n",
    "concatenated_tensor = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2,maxpool_3])\n",
    "flatten = Flatten()(concatenated_tensor)\n",
    "dropout = Dropout(drop)(flatten)\n",
    "output = Dense(units= N_CLASSES, activation='softmax')(dropout)\n",
    "\n",
    "# this creates a model that includes\n",
    "model_cnn = Model(inputs=sequence_input, outputs=output)\n",
    "\n",
    "checkpoint = ModelCheckpoint('weights_cnn_sentece.hdf5', monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n",
    "adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "model_cnn.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = './weights/factuality/cnn/weights_cnn-{epoch:02d}-{val_acc:.4f}.hdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(fp, monitor='val_acc', verbose=1, save_best_only=True, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2798 samples, validate on 494 samples\n",
      "Epoch 1/3\n",
      "  32/2798 [..............................] - ETA: 15s - loss: 0.2686 - acc: 0.9062"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2798/2798 [==============================] - 11s 4ms/step - loss: 0.2893 - acc: 0.8867 - val_loss: 0.7012 - val_acc: 0.7247\n",
      "\n",
      "Epoch 00001: val_acc did not improve from 0.74494\n",
      "Epoch 2/3\n",
      "2798/2798 [==============================] - 12s 4ms/step - loss: 0.2552 - acc: 0.9064 - val_loss: 0.7124 - val_acc: 0.7429\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.74494\n",
      "Epoch 3/3\n",
      "2798/2798 [==============================] - 13s 5ms/step - loss: 0.2482 - acc: 0.9114 - val_loss: 0.7019 - val_acc: 0.7470\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.74494 to 0.74696, saving model to ./weights/factuality/cnn/weights_cnn-03-0.7470.hdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12fb89ba8>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cnn.fit(x_train, Y_train_bin, validation_data=[x_test,Y_test_bin], nb_epoch=3, batch_size=16, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.87      0.85       250\n",
      "          1       0.67      0.47      0.55       119\n",
      "          2       0.64      0.76      0.70       125\n",
      "\n",
      "avg / total       0.74      0.75      0.74       494\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model_cnn.load_weights('./weights/factuality/cnn/weights_cnn-03-0.7470.hdf')\n",
    "\n",
    "y_pred = model_cnn.predict(x_test)\n",
    "\n",
    "output_test = np.argmax(y_pred, axis=1)\n",
    "print(classification_report(np.argmax(Y_test_bin, axis=1), output_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
